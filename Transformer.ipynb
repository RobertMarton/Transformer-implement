{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "uuid": "1859dc23-293d-4ef6-9024-e4dc3316ac56"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "class SingleScaledDotAttention():\n",
    "    def __init__(self, model_dim, ffn_dim):\n",
    "        self.linear_q = nn.Linear(model_dim, ffn_dim)\n",
    "        self.linear_k = nn.Linear(model_dim, ffn_dim)\n",
    "        self.linear_v = nn.Linear(model_dim, ffn_dim)\n",
    "    def forward(query, key, value, leng, scale):\n",
    "        query = self.linear_q(query)\n",
    "        key = self.linear_k(key)\n",
    "        value = self.linear_v(value)\n",
    "        attention = torch.bmm(query, key.transpose(1,2))\n",
    "        if scale:\n",
    "            scale = self.key.size(-1) ** -0.5\n",
    "            attention = torch.bmm(attention, scale)\n",
    "        mask = torch.zeros(attention.size(1), attention.size(-1))\n",
    "        for id_, len_ in enumerate(leng):\n",
    "            mask[id_][len_:] = 1 ###\n",
    "        attention.masked_fill_(mask.byte(), float('-inf'))\n",
    "        attention = nn.Softmax(attention, dim = 2)\n",
    "        context = torch.bmm(attention, value)\n",
    "        return attention, context\n",
    "        \n",
    "class scaledDotAttention(nn.Module):\n",
    "    def __init__(self, attentin_dropout=0.0):\n",
    "        super(scaledDotAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(attentin_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    def forward(self, query, key, value, attn_mask, scale):\n",
    "        attention = torch.bmm(query, key.transpose(1,2))\n",
    "#         if scale id not None:\n",
    "        print(attention.shape)\n",
    "        print(\"attn_mask:\", attn_mask)\n",
    "        attention = attention * scale #.cuda()\n",
    "        print(attention.shape)\n",
    "#         when use leng replace attn_mask:\n",
    "#         mask = torch.zeros(attention.size(0), attention.size(1), attention.size(-1))\n",
    "#         print(leng)\n",
    "#         for e_id, src_len in enumerate(leng):\n",
    "#             mask[e_id][src_len:][:] = 1\n",
    "#             for row in range(mask.size(1)):\n",
    "#                 mask[e_id][row][src_len:] = 1\n",
    "#         print(\"mask:\", mask)\n",
    "#         if attn_mask:\n",
    "        attention = attention.masked_fill_(attn_mask.byte(), float('-inf'))\n",
    "        print(\"attention:::\", attention)\n",
    "        attention = self.softmax(attention)\n",
    "        print(attention)\n",
    "        attention = self.dropout(attention)\n",
    "#         print(attention)\n",
    "        context = torch.bmm(attention, value)\n",
    "        return attention, context\n",
    "        \n",
    "            \n",
    "        \n",
    "class pointwiseFeedForward(nn.Module):\n",
    "    def __init__(self, model_dim=512, ffn_dim=2048, dropout=0.0):\n",
    "        super(pointwiseFeedForward, self).__init__()\n",
    "        self.W1 = nn.Conv1d(model_dim, ffn_dim, 1)\n",
    "#         self.W2 = nn.Conv1d(model_dim, ffn_dim, 1)\n",
    "        self.W2 = nn.Conv1d(ffn_dim, model_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "    def forward(self, X):\n",
    "        #[5,6,512]\n",
    "        print(X.shape)\n",
    "        output = F.relu(self.W1(X.transpose(1,2)))\n",
    "        output = self.W2(output)\n",
    "#         output = self.dropout(output.transpose(1,2))\n",
    "        output = output.transpose(1,2)\n",
    "        #add residual and norm layer\n",
    "        output = self.layer_norm(X + output)\n",
    "        print(\"sp:\", output.shape)\n",
    "        return output\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, model_dim=512, num_heads=8, dropout=0.0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dim_per_head = model_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        print(model_dim, self.dim_per_head)\n",
    "        self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads)#avoid float perheadDim\n",
    "        self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads)#avoid float perheadDim\n",
    "        self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads)#avoid float perheadDim\n",
    "        self.dot_product_attention = scaledDotAttention(dropout)\n",
    "        self.linear_final = nn.Linear(model_dim, model_dim)\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "    def forward(self, query, key, value, attn_mask=None):\n",
    "        residual = query\n",
    "        dim_per_head = self.dim_per_head\n",
    "        num_heads = self.num_heads\n",
    "        batch_size = query.size(0)\n",
    "        query = self.linear_q(query)\n",
    "        key = self.linear_q(key)\n",
    "        value = self.linear_q(value)\n",
    "        \n",
    "        #split by heads \n",
    "        query = query.view(batch_size * num_heads, -1, dim_per_head)\n",
    "        key = query.view(batch_size * num_heads, -1, dim_per_head)\n",
    "        value = query.view(batch_size * num_heads, -1, dim_per_head)\n",
    "        \n",
    "#         if attn_mask:\n",
    "        attn_mask = attn_mask.repeat(num_heads, 1, 1)\n",
    "        scale = key.size(-1) ** -0.5###diff?\n",
    "        attention, context = self.dot_product_attention(query, key, value, attn_mask, scale)\n",
    "        \n",
    "        #concat heads\n",
    "        context = context.view(batch_size, -1, dim_per_head*num_heads)\n",
    "        output = self.linear_final(context)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(residual + output)\n",
    "#         print(residual.shape, output.shape)\n",
    "        return attention, context\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim=512, num_heads=8, ffn_dim=2048, dropout=0.0 ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(model_dim, num_heads, dropout)\n",
    "        self.feed_forward = pointwiseFeedForward(model_dim, ffn_dim, dropout)\n",
    "    def forward(self, inputs, attn_mask=None):\n",
    "        leng = torch.tensor([6,5,4,3,2,1])\n",
    "        attention, context = self.attention(inputs, inputs, inputs, attn_mask)\n",
    "        output = self.feed_forward(context)\n",
    "        return output, attention\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position_encoding = np.array([\n",
    "            [pos / np.power(10000, 2.0*(j // 2) / d_model) for j in range(d_model)]\n",
    "            for pos in range(max_seq_len)\n",
    "            ])\n",
    "        position_encoding[:, 0::2] = np.sin(position_encoding[:, 0::2])\n",
    "        position_encoding[:, 1::2] = np.cos(position_encoding[:, 1::2])\n",
    "        position_encoding = torch.Tensor(position_encoding)\n",
    "        \n",
    "        pad_row = torch.zeros([1, d_model])\n",
    "        position_encoding = torch.cat((pad_row, position_encoding))\n",
    "        \n",
    "#         print(\"***))\", position_encoding.shape)\n",
    "        self.position_encoding = nn.Embedding(max_seq_len+1, d_model)\n",
    "        self.position_encoding.weight = nn.Parameter(position_encoding, requires_grad=False)\n",
    "\n",
    "    def forward(self, input_len):\n",
    "        print(\"***input_len:\", input_len)\n",
    "        #[bs, single_len_perSeq]\n",
    "        max_len = torch.max(input_len).item()\n",
    "        print(max_len)\n",
    "#         tensor = \n",
    "        input_pos = torch.tensor([list(range(1, len.item()+1)) + [0]*(max_len-len.item()) for len in input_len])\n",
    "        \n",
    "        print(\"***input_pos:\", input_pos)\n",
    "        return self.position_encoding(input_pos)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size=31,\n",
    "                 max_seq_len=3,\n",
    "                 num_layers=6,\n",
    "                 model_dim=512,\n",
    "                 num_heads=8,\n",
    "                 ffn_dim=2048,\n",
    "                 dropout=0.0\n",
    "                ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(model_dim, num_heads, ffn_dim, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "        self.seq_embedding = nn.Embedding(vocab_size+1, model_dim, padding_idx=0)\n",
    "        \n",
    "        self.pos_embedding = PositionalEncoding(model_dim, max_seq_len)\n",
    "#         self.pos_embedding = PositionalEncoding(model_dim, max_seq_len)\n",
    "\n",
    "    def forward(self, inputs, intpus_len):\n",
    "        def padding_mask(inputs, inputb):\n",
    "            print(\"inputs:\", inputs)\n",
    "            pad_mask = inputs.eq(0)\n",
    "            print(\"pad_mask:\", pad_mask)\n",
    "            len_q = inputb.size(1)\n",
    "            pad_mask = pad_mask.unsqueeze(1).expand(-1, len_q, -1)\n",
    "            print(\"pad_mask:\", pad_mask)\n",
    "            return pad_mask\n",
    "        output = self.seq_embedding(inputs)\n",
    "        output += self.pos_embedding(inputs_len)\n",
    "#         print(\"X:\", inputs)\n",
    "        self_attention_mask = padding_mask(inputs, inputs)\n",
    "        attentions = []\n",
    "        for encoder in self.encoder_layers:\n",
    "            output, attention = encoder(output, self_attention_mask)\n",
    "            attentions.append(attention)\n",
    "        return output, attentions\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "uuid": "0f8f38ab-dc9e-42d0-a878-b53135d469d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 64\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'repeat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-458b0d3d6865>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mencode1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#需要实例方法化（）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(\"X:\", X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-9e42e9706246>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, attn_mask)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mleng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-9e42e9706246>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, attn_mask)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m#         if attn_mask:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;31m###diff?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'repeat'"
     ]
    }
   ],
   "source": [
    "X = torch.rand(5,6,512)\n",
    "encode1 = EncoderLayer() #需要实例方法化（）\n",
    "output = encode1(X)\n",
    "# print(\"X:\", X)\n",
    "# print(output)\n",
    "# print(X.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "uuid": "8923dea2-9fcf-42ca-b986-d7e34e493a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 64\n",
      "512 64\n",
      "512 64\n",
      "512 64\n",
      "512 64\n",
      "512 64\n",
      "input: torch.Size([2, 3])\n",
      "inputs_len: torch.Size([2, 1])\n",
      "***input_len: tensor([[3],\n",
      "        [3]])\n",
      "3\n",
      "***input_pos: tensor([[1, 2, 3],\n",
      "        [1, 2, 3]])\n",
      "inputs: tensor([[1, 2, 0],\n",
      "        [1, 2, 0]])\n",
      "pad_mask: tensor([[0, 0, 1],\n",
      "        [0, 0, 1]], dtype=torch.uint8)\n",
      "pad_mask: tensor([[[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]]], dtype=torch.uint8)\n",
      "torch.Size([16, 3, 3])\n",
      "attn_mask: tensor([[[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]]], dtype=torch.uint8)\n",
      "torch.Size([16, 3, 3])\n",
      "attention::: tensor([[[ 5.6867, -0.2967,    -inf],\n",
      "         [-0.2967,  4.0867,    -inf],\n",
      "         [-0.0555,  0.0848,    -inf]],\n",
      "\n",
      "        [[ 4.1763, -0.5473,    -inf],\n",
      "         [-0.5473,  3.5093,    -inf],\n",
      "         [ 0.3030, -1.4206,    -inf]],\n",
      "\n",
      "        [[ 4.1114, -0.0455,    -inf],\n",
      "         [-0.0455,  3.6925,    -inf],\n",
      "         [ 0.6734, -0.3384,    -inf]],\n",
      "\n",
      "        [[ 3.7241,  0.0241,    -inf],\n",
      "         [ 0.0241,  2.8129,    -inf],\n",
      "         [-0.3361, -0.1735,    -inf]],\n",
      "\n",
      "        [[ 3.5889,  0.0181,    -inf],\n",
      "         [ 0.0181,  4.9560,    -inf],\n",
      "         [-0.4627, -0.2905,    -inf]],\n",
      "\n",
      "        [[ 5.1861,  0.7572,    -inf],\n",
      "         [ 0.7572,  1.3562,    -inf],\n",
      "         [ 0.4356, -0.1567,    -inf]],\n",
      "\n",
      "        [[ 0.9942,  0.0766,    -inf],\n",
      "         [ 0.0766,  1.4966,    -inf],\n",
      "         [ 0.0302,  0.4257,    -inf]],\n",
      "\n",
      "        [[ 1.5016, -0.0302,    -inf],\n",
      "         [-0.0302,  1.2294,    -inf],\n",
      "         [ 0.0239, -0.0477,    -inf]],\n",
      "\n",
      "        [[ 5.6867, -0.2967,    -inf],\n",
      "         [-0.2967,  4.0867,    -inf],\n",
      "         [-0.0555,  0.0848,    -inf]],\n",
      "\n",
      "        [[ 4.1763, -0.5473,    -inf],\n",
      "         [-0.5473,  3.5093,    -inf],\n",
      "         [ 0.3030, -1.4206,    -inf]],\n",
      "\n",
      "        [[ 4.1114, -0.0455,    -inf],\n",
      "         [-0.0455,  3.6925,    -inf],\n",
      "         [ 0.6734, -0.3384,    -inf]],\n",
      "\n",
      "        [[ 3.7241,  0.0241,    -inf],\n",
      "         [ 0.0241,  2.8129,    -inf],\n",
      "         [-0.3361, -0.1735,    -inf]],\n",
      "\n",
      "        [[ 3.5889,  0.0181,    -inf],\n",
      "         [ 0.0181,  4.9560,    -inf],\n",
      "         [-0.4627, -0.2905,    -inf]],\n",
      "\n",
      "        [[ 5.1861,  0.7572,    -inf],\n",
      "         [ 0.7572,  1.3562,    -inf],\n",
      "         [ 0.4356, -0.1567,    -inf]],\n",
      "\n",
      "        [[ 0.9942,  0.0766,    -inf],\n",
      "         [ 0.0766,  1.4966,    -inf],\n",
      "         [ 0.0302,  0.4257,    -inf]],\n",
      "\n",
      "        [[ 1.5016, -0.0302,    -inf],\n",
      "         [-0.0302,  1.2294,    -inf],\n",
      "         [ 0.0239, -0.0477,    -inf]]], grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[0.9975, 0.0025, 0.0000],\n",
      "         [0.0123, 0.9877, 0.0000],\n",
      "         [0.4650, 0.5350, 0.0000]],\n",
      "\n",
      "        [[0.9912, 0.0088, 0.0000],\n",
      "         [0.0170, 0.9830, 0.0000],\n",
      "         [0.8486, 0.1514, 0.0000]],\n",
      "\n",
      "        [[0.9846, 0.0154, 0.0000],\n",
      "         [0.0232, 0.9768, 0.0000],\n",
      "         [0.7334, 0.2666, 0.0000]],\n",
      "\n",
      "        [[0.9759, 0.0241, 0.0000],\n",
      "         [0.0579, 0.9421, 0.0000],\n",
      "         [0.4594, 0.5406, 0.0000]],\n",
      "\n",
      "        [[0.9726, 0.0274, 0.0000],\n",
      "         [0.0071, 0.9929, 0.0000],\n",
      "         [0.4571, 0.5429, 0.0000]],\n",
      "\n",
      "        [[0.9882, 0.0118, 0.0000],\n",
      "         [0.3546, 0.6454, 0.0000],\n",
      "         [0.6439, 0.3561, 0.0000]],\n",
      "\n",
      "        [[0.7145, 0.2855, 0.0000],\n",
      "         [0.1947, 0.8053, 0.0000],\n",
      "         [0.4024, 0.5976, 0.0000]],\n",
      "\n",
      "        [[0.8223, 0.1777, 0.0000],\n",
      "         [0.2210, 0.7790, 0.0000],\n",
      "         [0.5179, 0.4821, 0.0000]],\n",
      "\n",
      "        [[0.9975, 0.0025, 0.0000],\n",
      "         [0.0123, 0.9877, 0.0000],\n",
      "         [0.4650, 0.5350, 0.0000]],\n",
      "\n",
      "        [[0.9912, 0.0088, 0.0000],\n",
      "         [0.0170, 0.9830, 0.0000],\n",
      "         [0.8486, 0.1514, 0.0000]],\n",
      "\n",
      "        [[0.9846, 0.0154, 0.0000],\n",
      "         [0.0232, 0.9768, 0.0000],\n",
      "         [0.7334, 0.2666, 0.0000]],\n",
      "\n",
      "        [[0.9759, 0.0241, 0.0000],\n",
      "         [0.0579, 0.9421, 0.0000],\n",
      "         [0.4594, 0.5406, 0.0000]],\n",
      "\n",
      "        [[0.9726, 0.0274, 0.0000],\n",
      "         [0.0071, 0.9929, 0.0000],\n",
      "         [0.4571, 0.5429, 0.0000]],\n",
      "\n",
      "        [[0.9882, 0.0118, 0.0000],\n",
      "         [0.3546, 0.6454, 0.0000],\n",
      "         [0.6439, 0.3561, 0.0000]],\n",
      "\n",
      "        [[0.7145, 0.2855, 0.0000],\n",
      "         [0.1947, 0.8053, 0.0000],\n",
      "         [0.4024, 0.5976, 0.0000]],\n",
      "\n",
      "        [[0.8223, 0.1777, 0.0000],\n",
      "         [0.2210, 0.7790, 0.0000],\n",
      "         [0.5179, 0.4821, 0.0000]]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([2, 3, 512])\n",
      "sp: torch.Size([2, 3, 512])\n",
      "torch.Size([16, 3, 3])\n",
      "attn_mask: tensor([[[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]]], dtype=torch.uint8)\n",
      "torch.Size([16, 3, 3])\n",
      "attention::: tensor([[[ 2.1736, -0.3227,    -inf],\n",
      "         [-0.3227,  3.9894,    -inf],\n",
      "         [-0.0988, -0.1696,    -inf]],\n",
      "\n",
      "        [[ 3.6044,  0.1450,    -inf],\n",
      "         [ 0.1450,  1.8700,    -inf],\n",
      "         [ 0.3153, -0.1473,    -inf]],\n",
      "\n",
      "        [[ 3.4442, -0.8238,    -inf],\n",
      "         [-0.8238,  2.9668,    -inf],\n",
      "         [-0.1249, -0.3766,    -inf]],\n",
      "\n",
      "        [[ 2.2206, -0.3523,    -inf],\n",
      "         [-0.3523,  2.2118,    -inf],\n",
      "         [ 0.4718, -0.6611,    -inf]],\n",
      "\n",
      "        [[ 2.9564,  0.2745,    -inf],\n",
      "         [ 0.2745,  1.8765,    -inf],\n",
      "         [ 0.1538, -0.1535,    -inf]],\n",
      "\n",
      "        [[ 2.4450, -0.2103,    -inf],\n",
      "         [-0.2103,  2.7241,    -inf],\n",
      "         [ 0.1535,  0.5177,    -inf]],\n",
      "\n",
      "        [[ 2.6904, -0.1111,    -inf],\n",
      "         [-0.1111,  2.3396,    -inf],\n",
      "         [ 0.0764,  0.1846,    -inf]],\n",
      "\n",
      "        [[ 2.2726,  0.2886,    -inf],\n",
      "         [ 0.2886,  3.0982,    -inf],\n",
      "         [ 0.0624, -0.4746,    -inf]],\n",
      "\n",
      "        [[ 2.1736, -0.3227,    -inf],\n",
      "         [-0.3227,  3.9894,    -inf],\n",
      "         [-0.0988, -0.1696,    -inf]],\n",
      "\n",
      "        [[ 3.6044,  0.1450,    -inf],\n",
      "         [ 0.1450,  1.8700,    -inf],\n",
      "         [ 0.3153, -0.1473,    -inf]],\n",
      "\n",
      "        [[ 3.4442, -0.8238,    -inf],\n",
      "         [-0.8238,  2.9668,    -inf],\n",
      "         [-0.1249, -0.3766,    -inf]],\n",
      "\n",
      "        [[ 2.2206, -0.3523,    -inf],\n",
      "         [-0.3523,  2.2118,    -inf],\n",
      "         [ 0.4718, -0.6611,    -inf]],\n",
      "\n",
      "        [[ 2.9564,  0.2745,    -inf],\n",
      "         [ 0.2745,  1.8765,    -inf],\n",
      "         [ 0.1538, -0.1535,    -inf]],\n",
      "\n",
      "        [[ 2.4450, -0.2103,    -inf],\n",
      "         [-0.2103,  2.7241,    -inf],\n",
      "         [ 0.1535,  0.5177,    -inf]],\n",
      "\n",
      "        [[ 2.6904, -0.1111,    -inf],\n",
      "         [-0.1111,  2.3396,    -inf],\n",
      "         [ 0.0764,  0.1846,    -inf]],\n",
      "\n",
      "        [[ 2.2726,  0.2886,    -inf],\n",
      "         [ 0.2886,  3.0982,    -inf],\n",
      "         [ 0.0624, -0.4746,    -inf]]], grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[0.9239, 0.0761, 0.0000],\n",
      "         [0.0132, 0.9868, 0.0000],\n",
      "         [0.5177, 0.4823, 0.0000]],\n",
      "\n",
      "        [[0.9695, 0.0305, 0.0000],\n",
      "         [0.1512, 0.8488, 0.0000],\n",
      "         [0.6136, 0.3864, 0.0000]],\n",
      "\n",
      "        [[0.9862, 0.0138, 0.0000],\n",
      "         [0.0221, 0.9779, 0.0000],\n",
      "         [0.5626, 0.4374, 0.0000]],\n",
      "\n",
      "        [[0.9291, 0.0709, 0.0000],\n",
      "         [0.0715, 0.9285, 0.0000],\n",
      "         [0.7564, 0.2436, 0.0000]],\n",
      "\n",
      "        [[0.9360, 0.0640, 0.0000],\n",
      "         [0.1677, 0.8323, 0.0000],\n",
      "         [0.5762, 0.4238, 0.0000]],\n",
      "\n",
      "        [[0.9343, 0.0657, 0.0000],\n",
      "         [0.0505, 0.9495, 0.0000],\n",
      "         [0.4099, 0.5901, 0.0000]],\n",
      "\n",
      "        [[0.9428, 0.0572, 0.0000],\n",
      "         [0.0794, 0.9206, 0.0000],\n",
      "         [0.4730, 0.5270, 0.0000]],\n",
      "\n",
      "        [[0.8791, 0.1209, 0.0000],\n",
      "         [0.0568, 0.9432, 0.0000],\n",
      "         [0.6311, 0.3689, 0.0000]],\n",
      "\n",
      "        [[0.9239, 0.0761, 0.0000],\n",
      "         [0.0132, 0.9868, 0.0000],\n",
      "         [0.5177, 0.4823, 0.0000]],\n",
      "\n",
      "        [[0.9695, 0.0305, 0.0000],\n",
      "         [0.1512, 0.8488, 0.0000],\n",
      "         [0.6136, 0.3864, 0.0000]],\n",
      "\n",
      "        [[0.9862, 0.0138, 0.0000],\n",
      "         [0.0221, 0.9779, 0.0000],\n",
      "         [0.5626, 0.4374, 0.0000]],\n",
      "\n",
      "        [[0.9291, 0.0709, 0.0000],\n",
      "         [0.0715, 0.9285, 0.0000],\n",
      "         [0.7564, 0.2436, 0.0000]],\n",
      "\n",
      "        [[0.9360, 0.0640, 0.0000],\n",
      "         [0.1677, 0.8323, 0.0000],\n",
      "         [0.5762, 0.4238, 0.0000]],\n",
      "\n",
      "        [[0.9343, 0.0657, 0.0000],\n",
      "         [0.0505, 0.9495, 0.0000],\n",
      "         [0.4099, 0.5901, 0.0000]],\n",
      "\n",
      "        [[0.9428, 0.0572, 0.0000],\n",
      "         [0.0794, 0.9206, 0.0000],\n",
      "         [0.4730, 0.5270, 0.0000]],\n",
      "\n",
      "        [[0.8791, 0.1209, 0.0000],\n",
      "         [0.0568, 0.9432, 0.0000],\n",
      "         [0.6311, 0.3689, 0.0000]]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([2, 3, 512])\n",
      "sp: torch.Size([2, 3, 512])\n",
      "torch.Size([16, 3, 3])\n",
      "attn_mask: tensor([[[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]]], dtype=torch.uint8)\n",
      "torch.Size([16, 3, 3])\n",
      "attention::: tensor([[[ 2.2801, -0.4007,    -inf],\n",
      "         [-0.4007,  2.5772,    -inf],\n",
      "         [-0.2271,  0.3806,    -inf]],\n",
      "\n",
      "        [[ 2.5629, -0.5248,    -inf],\n",
      "         [-0.5248,  3.1822,    -inf],\n",
      "         [-0.3113,  0.5373,    -inf]],\n",
      "\n",
      "        [[ 3.0805,  0.7715,    -inf],\n",
      "         [ 0.7715,  3.0306,    -inf],\n",
      "         [ 0.4500,  0.8063,    -inf]],\n",
      "\n",
      "        [[ 1.9448,  0.4692,    -inf],\n",
      "         [ 0.4692,  3.4833,    -inf],\n",
      "         [-0.5428, -0.4947,    -inf]],\n",
      "\n",
      "        [[ 2.3129, -0.0408,    -inf],\n",
      "         [-0.0408,  2.6606,    -inf],\n",
      "         [-0.0590,  0.3162,    -inf]],\n",
      "\n",
      "        [[ 2.8532,  0.2105,    -inf],\n",
      "         [ 0.2105,  2.2122,    -inf],\n",
      "         [ 0.5803, -0.2038,    -inf]],\n",
      "\n",
      "        [[ 1.9493, -0.0893,    -inf],\n",
      "         [-0.0893,  2.0732,    -inf],\n",
      "         [ 0.3688,  0.1648,    -inf]],\n",
      "\n",
      "        [[ 2.5238, -0.0329,    -inf],\n",
      "         [-0.0329,  2.6462,    -inf],\n",
      "         [ 0.3312,  0.3430,    -inf]],\n",
      "\n",
      "        [[ 2.2801, -0.4007,    -inf],\n",
      "         [-0.4007,  2.5772,    -inf],\n",
      "         [-0.2271,  0.3806,    -inf]],\n",
      "\n",
      "        [[ 2.5629, -0.5248,    -inf],\n",
      "         [-0.5248,  3.1822,    -inf],\n",
      "         [-0.3113,  0.5373,    -inf]],\n",
      "\n",
      "        [[ 3.0805,  0.7715,    -inf],\n",
      "         [ 0.7715,  3.0306,    -inf],\n",
      "         [ 0.4500,  0.8063,    -inf]],\n",
      "\n",
      "        [[ 1.9448,  0.4692,    -inf],\n",
      "         [ 0.4692,  3.4833,    -inf],\n",
      "         [-0.5428, -0.4947,    -inf]],\n",
      "\n",
      "        [[ 2.3129, -0.0408,    -inf],\n",
      "         [-0.0408,  2.6606,    -inf],\n",
      "         [-0.0590,  0.3162,    -inf]],\n",
      "\n",
      "        [[ 2.8532,  0.2105,    -inf],\n",
      "         [ 0.2105,  2.2122,    -inf],\n",
      "         [ 0.5803, -0.2038,    -inf]],\n",
      "\n",
      "        [[ 1.9493, -0.0893,    -inf],\n",
      "         [-0.0893,  2.0732,    -inf],\n",
      "         [ 0.3688,  0.1648,    -inf]],\n",
      "\n",
      "        [[ 2.5238, -0.0329,    -inf],\n",
      "         [-0.0329,  2.6462,    -inf],\n",
      "         [ 0.3312,  0.3430,    -inf]]], grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[0.9359, 0.0641, 0.0000],\n",
      "         [0.0484, 0.9516, 0.0000],\n",
      "         [0.3526, 0.6474, 0.0000]],\n",
      "\n",
      "        [[0.9564, 0.0436, 0.0000],\n",
      "         [0.0240, 0.9760, 0.0000],\n",
      "         [0.2997, 0.7003, 0.0000]],\n",
      "\n",
      "        [[0.9096, 0.0904, 0.0000],\n",
      "         [0.0946, 0.9054, 0.0000],\n",
      "         [0.4119, 0.5881, 0.0000]],\n",
      "\n",
      "        [[0.8139, 0.1861, 0.0000],\n",
      "         [0.0468, 0.9532, 0.0000],\n",
      "         [0.4880, 0.5120, 0.0000]],\n",
      "\n",
      "        [[0.9132, 0.0868, 0.0000],\n",
      "         [0.0629, 0.9371, 0.0000],\n",
      "         [0.4073, 0.5927, 0.0000]],\n",
      "\n",
      "        [[0.9336, 0.0664, 0.0000],\n",
      "         [0.1190, 0.8810, 0.0000],\n",
      "         [0.6866, 0.3134, 0.0000]],\n",
      "\n",
      "        [[0.8848, 0.1152, 0.0000],\n",
      "         [0.1032, 0.8968, 0.0000],\n",
      "         [0.5508, 0.4492, 0.0000]],\n",
      "\n",
      "        [[0.9280, 0.0720, 0.0000],\n",
      "         [0.0642, 0.9358, 0.0000],\n",
      "         [0.4971, 0.5029, 0.0000]],\n",
      "\n",
      "        [[0.9359, 0.0641, 0.0000],\n",
      "         [0.0484, 0.9516, 0.0000],\n",
      "         [0.3526, 0.6474, 0.0000]],\n",
      "\n",
      "        [[0.9564, 0.0436, 0.0000],\n",
      "         [0.0240, 0.9760, 0.0000],\n",
      "         [0.2997, 0.7003, 0.0000]],\n",
      "\n",
      "        [[0.9096, 0.0904, 0.0000],\n",
      "         [0.0946, 0.9054, 0.0000],\n",
      "         [0.4119, 0.5881, 0.0000]],\n",
      "\n",
      "        [[0.8139, 0.1861, 0.0000],\n",
      "         [0.0468, 0.9532, 0.0000],\n",
      "         [0.4880, 0.5120, 0.0000]],\n",
      "\n",
      "        [[0.9132, 0.0868, 0.0000],\n",
      "         [0.0629, 0.9371, 0.0000],\n",
      "         [0.4073, 0.5927, 0.0000]],\n",
      "\n",
      "        [[0.9336, 0.0664, 0.0000],\n",
      "         [0.1190, 0.8810, 0.0000],\n",
      "         [0.6866, 0.3134, 0.0000]],\n",
      "\n",
      "        [[0.8848, 0.1152, 0.0000],\n",
      "         [0.1032, 0.8968, 0.0000],\n",
      "         [0.5508, 0.4492, 0.0000]],\n",
      "\n",
      "        [[0.9280, 0.0720, 0.0000],\n",
      "         [0.0642, 0.9358, 0.0000],\n",
      "         [0.4971, 0.5029, 0.0000]]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([2, 3, 512])\n",
      "sp: torch.Size([2, 3, 512])\n",
      "torch.Size([16, 3, 3])\n",
      "attn_mask: tensor([[[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]]], dtype=torch.uint8)\n",
      "torch.Size([16, 3, 3])\n",
      "attention::: tensor([[[ 2.7681, -0.1598,    -inf],\n",
      "         [-0.1598,  3.3039,    -inf],\n",
      "         [ 0.1179,  0.4087,    -inf]],\n",
      "\n",
      "        [[ 3.4234, -0.1652,    -inf],\n",
      "         [-0.1652,  2.6288,    -inf],\n",
      "         [-0.6556,  0.1400,    -inf]],\n",
      "\n",
      "        [[ 3.5091,  0.3799,    -inf],\n",
      "         [ 0.3799,  2.5757,    -inf],\n",
      "         [-0.3744,  0.1043,    -inf]],\n",
      "\n",
      "        [[ 2.8446,  0.3606,    -inf],\n",
      "         [ 0.3606,  2.7916,    -inf],\n",
      "         [-0.1021,  0.0462,    -inf]],\n",
      "\n",
      "        [[ 2.8145, -0.3118,    -inf],\n",
      "         [-0.3118,  3.0583,    -inf],\n",
      "         [ 0.0477, -0.1089,    -inf]],\n",
      "\n",
      "        [[ 2.6294, -0.1017,    -inf],\n",
      "         [-0.1017,  1.9323,    -inf],\n",
      "         [-0.2841,  0.1849,    -inf]],\n",
      "\n",
      "        [[ 2.7594,  0.7499,    -inf],\n",
      "         [ 0.7499,  2.5897,    -inf],\n",
      "         [-0.3705, -0.2221,    -inf]],\n",
      "\n",
      "        [[ 2.7091, -0.5222,    -inf],\n",
      "         [-0.5222,  3.1476,    -inf],\n",
      "         [-0.1940,  0.5993,    -inf]],\n",
      "\n",
      "        [[ 2.7681, -0.1598,    -inf],\n",
      "         [-0.1598,  3.3039,    -inf],\n",
      "         [ 0.1179,  0.4087,    -inf]],\n",
      "\n",
      "        [[ 3.4234, -0.1652,    -inf],\n",
      "         [-0.1652,  2.6288,    -inf],\n",
      "         [-0.6556,  0.1400,    -inf]],\n",
      "\n",
      "        [[ 3.5091,  0.3799,    -inf],\n",
      "         [ 0.3799,  2.5757,    -inf],\n",
      "         [-0.3744,  0.1043,    -inf]],\n",
      "\n",
      "        [[ 2.8446,  0.3606,    -inf],\n",
      "         [ 0.3606,  2.7916,    -inf],\n",
      "         [-0.1021,  0.0462,    -inf]],\n",
      "\n",
      "        [[ 2.8145, -0.3118,    -inf],\n",
      "         [-0.3118,  3.0583,    -inf],\n",
      "         [ 0.0477, -0.1089,    -inf]],\n",
      "\n",
      "        [[ 2.6294, -0.1017,    -inf],\n",
      "         [-0.1017,  1.9323,    -inf],\n",
      "         [-0.2841,  0.1849,    -inf]],\n",
      "\n",
      "        [[ 2.7594,  0.7499,    -inf],\n",
      "         [ 0.7499,  2.5897,    -inf],\n",
      "         [-0.3705, -0.2221,    -inf]],\n",
      "\n",
      "        [[ 2.7091, -0.5222,    -inf],\n",
      "         [-0.5222,  3.1476,    -inf],\n",
      "         [-0.1940,  0.5993,    -inf]]], grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[0.9492, 0.0508, 0.0000],\n",
      "         [0.0304, 0.9696, 0.0000],\n",
      "         [0.4278, 0.5722, 0.0000]],\n",
      "\n",
      "        [[0.9731, 0.0269, 0.0000],\n",
      "         [0.0577, 0.9423, 0.0000],\n",
      "         [0.3110, 0.6890, 0.0000]],\n",
      "\n",
      "        [[0.9581, 0.0419, 0.0000],\n",
      "         [0.1001, 0.8999, 0.0000],\n",
      "         [0.3826, 0.6174, 0.0000]],\n",
      "\n",
      "        [[0.9230, 0.0770, 0.0000],\n",
      "         [0.0808, 0.9192, 0.0000],\n",
      "         [0.4630, 0.5370, 0.0000]],\n",
      "\n",
      "        [[0.9580, 0.0420, 0.0000],\n",
      "         [0.0332, 0.9668, 0.0000],\n",
      "         [0.5391, 0.4609, 0.0000]],\n",
      "\n",
      "        [[0.9388, 0.0612, 0.0000],\n",
      "         [0.1157, 0.8843, 0.0000],\n",
      "         [0.3848, 0.6152, 0.0000]],\n",
      "\n",
      "        [[0.8818, 0.1182, 0.0000],\n",
      "         [0.1371, 0.8629, 0.0000],\n",
      "         [0.4630, 0.5370, 0.0000]],\n",
      "\n",
      "        [[0.9620, 0.0380, 0.0000],\n",
      "         [0.0248, 0.9752, 0.0000],\n",
      "         [0.3115, 0.6885, 0.0000]],\n",
      "\n",
      "        [[0.9492, 0.0508, 0.0000],\n",
      "         [0.0304, 0.9696, 0.0000],\n",
      "         [0.4278, 0.5722, 0.0000]],\n",
      "\n",
      "        [[0.9731, 0.0269, 0.0000],\n",
      "         [0.0577, 0.9423, 0.0000],\n",
      "         [0.3110, 0.6890, 0.0000]],\n",
      "\n",
      "        [[0.9581, 0.0419, 0.0000],\n",
      "         [0.1001, 0.8999, 0.0000],\n",
      "         [0.3826, 0.6174, 0.0000]],\n",
      "\n",
      "        [[0.9230, 0.0770, 0.0000],\n",
      "         [0.0808, 0.9192, 0.0000],\n",
      "         [0.4630, 0.5370, 0.0000]],\n",
      "\n",
      "        [[0.9580, 0.0420, 0.0000],\n",
      "         [0.0332, 0.9668, 0.0000],\n",
      "         [0.5391, 0.4609, 0.0000]],\n",
      "\n",
      "        [[0.9388, 0.0612, 0.0000],\n",
      "         [0.1157, 0.8843, 0.0000],\n",
      "         [0.3848, 0.6152, 0.0000]],\n",
      "\n",
      "        [[0.8818, 0.1182, 0.0000],\n",
      "         [0.1371, 0.8629, 0.0000],\n",
      "         [0.4630, 0.5370, 0.0000]],\n",
      "\n",
      "        [[0.9620, 0.0380, 0.0000],\n",
      "         [0.0248, 0.9752, 0.0000],\n",
      "         [0.3115, 0.6885, 0.0000]]], grad_fn=<SoftmaxBackward>)\n",
      "sp: torch.Size([2, 3, 512])\n",
      "torch.Size([16, 3, 3])\n",
      "attn_mask: tensor([[[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]]], dtype=torch.uint8)\n",
      "torch.Size([16, 3, 3])\n",
      "attention::: tensor([[[ 2.7794e+00, -1.9737e-01,        -inf],\n",
      "         [-1.9737e-01,  2.6311e+00,        -inf],\n",
      "         [-1.2265e-01,  4.5957e-03,        -inf]],\n",
      "\n",
      "        [[ 3.7301e+00, -3.8764e-01,        -inf],\n",
      "         [-3.8764e-01,  2.7597e+00,        -inf],\n",
      "         [ 3.3862e-02, -4.9711e-01,        -inf]],\n",
      "\n",
      "        [[ 3.0080e+00,  3.7067e-02,        -inf],\n",
      "         [ 3.7067e-02,  2.5453e+00,        -inf],\n",
      "         [ 4.1773e-02,  1.5171e-02,        -inf]],\n",
      "\n",
      "        [[ 2.5736e+00, -2.3024e-01,        -inf],\n",
      "         [-2.3024e-01,  3.2283e+00,        -inf],\n",
      "         [-1.0712e-01, -2.1398e-01,        -inf]],\n",
      "\n",
      "        [[ 2.6282e+00,  5.2138e-01,        -inf],\n",
      "         [ 5.2138e-01,  2.4095e+00,        -inf],\n",
      "         [ 2.4894e-03,  7.2613e-02,        -inf]],\n",
      "\n",
      "        [[ 2.5172e+00, -1.9699e-01,        -inf],\n",
      "         [-1.9699e-01,  2.9570e+00,        -inf],\n",
      "         [-4.4349e-01,  1.2860e-01,        -inf]],\n",
      "\n",
      "        [[ 2.7018e+00, -5.9251e-01,        -inf],\n",
      "         [-5.9251e-01,  3.7173e+00,        -inf],\n",
      "         [ 2.6977e-01, -1.3524e-01,        -inf]],\n",
      "\n",
      "        [[ 2.4341e+00,  1.2605e-01,        -inf],\n",
      "         [ 1.2605e-01,  2.9688e+00,        -inf],\n",
      "         [ 1.5632e-02,  2.0699e-01,        -inf]],\n",
      "\n",
      "        [[ 2.7794e+00, -1.9737e-01,        -inf],\n",
      "         [-1.9737e-01,  2.6311e+00,        -inf],\n",
      "         [-1.2265e-01,  4.5957e-03,        -inf]],\n",
      "\n",
      "        [[ 3.7301e+00, -3.8764e-01,        -inf],\n",
      "         [-3.8764e-01,  2.7597e+00,        -inf],\n",
      "         [ 3.3862e-02, -4.9711e-01,        -inf]],\n",
      "\n",
      "        [[ 3.0080e+00,  3.7067e-02,        -inf],\n",
      "         [ 3.7067e-02,  2.5453e+00,        -inf],\n",
      "         [ 4.1773e-02,  1.5171e-02,        -inf]],\n",
      "\n",
      "        [[ 2.5736e+00, -2.3024e-01,        -inf],\n",
      "         [-2.3024e-01,  3.2283e+00,        -inf],\n",
      "         [-1.0712e-01, -2.1398e-01,        -inf]],\n",
      "\n",
      "        [[ 2.6282e+00,  5.2138e-01,        -inf],\n",
      "         [ 5.2138e-01,  2.4095e+00,        -inf],\n",
      "         [ 2.4894e-03,  7.2613e-02,        -inf]],\n",
      "\n",
      "        [[ 2.5172e+00, -1.9699e-01,        -inf],\n",
      "         [-1.9699e-01,  2.9570e+00,        -inf],\n",
      "         [-4.4349e-01,  1.2860e-01,        -inf]],\n",
      "\n",
      "        [[ 2.7018e+00, -5.9251e-01,        -inf],\n",
      "         [-5.9251e-01,  3.7173e+00,        -inf],\n",
      "         [ 2.6977e-01, -1.3524e-01,        -inf]],\n",
      "\n",
      "        [[ 2.4341e+00,  1.2605e-01,        -inf],\n",
      "         [ 1.2605e-01,  2.9688e+00,        -inf],\n",
      "         [ 1.5632e-02,  2.0699e-01,        -inf]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[0.9515, 0.0485, 0.0000],\n",
      "         [0.0558, 0.9442, 0.0000],\n",
      "         [0.4682, 0.5318, 0.0000]],\n",
      "\n",
      "        [[0.9840, 0.0160, 0.0000],\n",
      "         [0.0412, 0.9588, 0.0000],\n",
      "         [0.6297, 0.3703, 0.0000]],\n",
      "\n",
      "        [[0.9512, 0.0488, 0.0000],\n",
      "         [0.0753, 0.9247, 0.0000],\n",
      "         [0.5067, 0.4933, 0.0000]],\n",
      "\n",
      "        [[0.9429, 0.0571, 0.0000],\n",
      "         [0.0305, 0.9695, 0.0000],\n",
      "         [0.5267, 0.4733, 0.0000]],\n",
      "\n",
      "        [[0.8916, 0.1084, 0.0000],\n",
      "         [0.1315, 0.8685, 0.0000],\n",
      "         [0.4825, 0.5175, 0.0000]],\n",
      "\n",
      "        [[0.9379, 0.0621, 0.0000],\n",
      "         [0.0409, 0.9591, 0.0000],\n",
      "         [0.3608, 0.6392, 0.0000]],\n",
      "\n",
      "        [[0.9642, 0.0358, 0.0000],\n",
      "         [0.0133, 0.9867, 0.0000],\n",
      "         [0.5999, 0.4001, 0.0000]],\n",
      "\n",
      "        [[0.9095, 0.0905, 0.0000],\n",
      "         [0.0551, 0.9449, 0.0000],\n",
      "         [0.4523, 0.5477, 0.0000]],\n",
      "\n",
      "        [[0.9515, 0.0485, 0.0000],\n",
      "         [0.0558, 0.9442, 0.0000],\n",
      "         [0.4682, 0.5318, 0.0000]],\n",
      "\n",
      "        [[0.9840, 0.0160, 0.0000],\n",
      "         [0.0412, 0.9588, 0.0000],\n",
      "         [0.6297, 0.3703, 0.0000]],\n",
      "\n",
      "        [[0.9512, 0.0488, 0.0000],\n",
      "         [0.0753, 0.9247, 0.0000],\n",
      "         [0.5067, 0.4933, 0.0000]],\n",
      "\n",
      "        [[0.9429, 0.0571, 0.0000],\n",
      "         [0.0305, 0.9695, 0.0000],\n",
      "         [0.5267, 0.4733, 0.0000]],\n",
      "\n",
      "        [[0.8916, 0.1084, 0.0000],\n",
      "         [0.1315, 0.8685, 0.0000],\n",
      "         [0.4825, 0.5175, 0.0000]],\n",
      "\n",
      "        [[0.9379, 0.0621, 0.0000],\n",
      "         [0.0409, 0.9591, 0.0000],\n",
      "         [0.3608, 0.6392, 0.0000]],\n",
      "\n",
      "        [[0.9642, 0.0358, 0.0000],\n",
      "         [0.0133, 0.9867, 0.0000],\n",
      "         [0.5999, 0.4001, 0.0000]],\n",
      "\n",
      "        [[0.9095, 0.0905, 0.0000],\n",
      "         [0.0551, 0.9449, 0.0000],\n",
      "         [0.4523, 0.5477, 0.0000]]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([2, 3, 512])\n",
      "sp: torch.Size([2, 3, 512])\n",
      "torch.Size([16, 3, 3])\n",
      "attn_mask: tensor([[[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]]], dtype=torch.uint8)\n",
      "torch.Size([16, 3, 3])\n",
      "attention::: tensor([[[ 2.7772e+00,  6.4104e-01,        -inf],\n",
      "         [ 6.4104e-01,  3.4011e+00,        -inf],\n",
      "         [ 2.4306e-01, -1.8856e-01,        -inf]],\n",
      "\n",
      "        [[ 3.4860e+00, -4.7411e-02,        -inf],\n",
      "         [-4.7411e-02,  2.5071e+00,        -inf],\n",
      "         [-9.7585e-02,  1.7920e-02,        -inf]],\n",
      "\n",
      "        [[ 2.4776e+00,  6.1488e-03,        -inf],\n",
      "         [ 6.1488e-03,  2.4058e+00,        -inf],\n",
      "         [-8.1513e-01, -5.2062e-01,        -inf]],\n",
      "\n",
      "        [[ 3.2855e+00, -2.5701e-03,        -inf],\n",
      "         [-2.5701e-03,  2.2866e+00,        -inf],\n",
      "         [-3.6422e-01, -5.2974e-02,        -inf]],\n",
      "\n",
      "        [[ 2.1559e+00,  2.3110e-01,        -inf],\n",
      "         [ 2.3110e-01,  3.0414e+00,        -inf],\n",
      "         [-2.0487e-02,  4.6099e-01,        -inf]],\n",
      "\n",
      "        [[ 2.7277e+00, -3.9305e-01,        -inf],\n",
      "         [-3.9305e-01,  3.0838e+00,        -inf],\n",
      "         [-2.7948e-01,  3.5673e-01,        -inf]],\n",
      "\n",
      "        [[ 2.0262e+00, -6.6446e-02,        -inf],\n",
      "         [-6.6446e-02,  3.2889e+00,        -inf],\n",
      "         [-1.4197e-01,  8.7451e-03,        -inf]],\n",
      "\n",
      "        [[ 2.3890e+00, -2.9012e-01,        -inf],\n",
      "         [-2.9012e-01,  2.1781e+00,        -inf],\n",
      "         [ 2.3073e-01,  1.9414e-01,        -inf]],\n",
      "\n",
      "        [[ 2.7772e+00,  6.4104e-01,        -inf],\n",
      "         [ 6.4104e-01,  3.4011e+00,        -inf],\n",
      "         [ 2.4306e-01, -1.8856e-01,        -inf]],\n",
      "\n",
      "        [[ 3.4860e+00, -4.7411e-02,        -inf],\n",
      "         [-4.7411e-02,  2.5071e+00,        -inf],\n",
      "         [-9.7585e-02,  1.7920e-02,        -inf]],\n",
      "\n",
      "        [[ 2.4776e+00,  6.1488e-03,        -inf],\n",
      "         [ 6.1488e-03,  2.4058e+00,        -inf],\n",
      "         [-8.1513e-01, -5.2062e-01,        -inf]],\n",
      "\n",
      "        [[ 3.2855e+00, -2.5701e-03,        -inf],\n",
      "         [-2.5701e-03,  2.2866e+00,        -inf],\n",
      "         [-3.6422e-01, -5.2974e-02,        -inf]],\n",
      "\n",
      "        [[ 2.1559e+00,  2.3110e-01,        -inf],\n",
      "         [ 2.3110e-01,  3.0414e+00,        -inf],\n",
      "         [-2.0487e-02,  4.6099e-01,        -inf]],\n",
      "\n",
      "        [[ 2.7277e+00, -3.9305e-01,        -inf],\n",
      "         [-3.9305e-01,  3.0838e+00,        -inf],\n",
      "         [-2.7948e-01,  3.5673e-01,        -inf]],\n",
      "\n",
      "        [[ 2.0262e+00, -6.6446e-02,        -inf],\n",
      "         [-6.6446e-02,  3.2889e+00,        -inf],\n",
      "         [-1.4197e-01,  8.7451e-03,        -inf]],\n",
      "\n",
      "        [[ 2.3890e+00, -2.9012e-01,        -inf],\n",
      "         [-2.9012e-01,  2.1781e+00,        -inf],\n",
      "         [ 2.3073e-01,  1.9414e-01,        -inf]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[0.8944, 0.1056, 0.0000],\n",
      "         [0.0595, 0.9405, 0.0000],\n",
      "         [0.6063, 0.3937, 0.0000]],\n",
      "\n",
      "        [[0.9716, 0.0284, 0.0000],\n",
      "         [0.0721, 0.9279, 0.0000],\n",
      "         [0.4712, 0.5288, 0.0000]],\n",
      "\n",
      "        [[0.9221, 0.0779, 0.0000],\n",
      "         [0.0832, 0.9168, 0.0000],\n",
      "         [0.4269, 0.5731, 0.0000]],\n",
      "\n",
      "        [[0.9640, 0.0360, 0.0000],\n",
      "         [0.0920, 0.9080, 0.0000],\n",
      "         [0.4228, 0.5772, 0.0000]],\n",
      "\n",
      "        [[0.8727, 0.1273, 0.0000],\n",
      "         [0.0568, 0.9432, 0.0000],\n",
      "         [0.3819, 0.6181, 0.0000]],\n",
      "\n",
      "        [[0.9577, 0.0423, 0.0000],\n",
      "         [0.0300, 0.9700, 0.0000],\n",
      "         [0.3461, 0.6539, 0.0000]],\n",
      "\n",
      "        [[0.8902, 0.1098, 0.0000],\n",
      "         [0.0337, 0.9663, 0.0000],\n",
      "         [0.4624, 0.5376, 0.0000]],\n",
      "\n",
      "        [[0.9358, 0.0642, 0.0000],\n",
      "         [0.0781, 0.9219, 0.0000],\n",
      "         [0.5091, 0.4909, 0.0000]],\n",
      "\n",
      "        [[0.8944, 0.1056, 0.0000],\n",
      "         [0.0595, 0.9405, 0.0000],\n",
      "         [0.6063, 0.3937, 0.0000]],\n",
      "\n",
      "        [[0.9716, 0.0284, 0.0000],\n",
      "         [0.0721, 0.9279, 0.0000],\n",
      "         [0.4712, 0.5288, 0.0000]],\n",
      "\n",
      "        [[0.9221, 0.0779, 0.0000],\n",
      "         [0.0832, 0.9168, 0.0000],\n",
      "         [0.4269, 0.5731, 0.0000]],\n",
      "\n",
      "        [[0.9640, 0.0360, 0.0000],\n",
      "         [0.0920, 0.9080, 0.0000],\n",
      "         [0.4228, 0.5772, 0.0000]],\n",
      "\n",
      "        [[0.8727, 0.1273, 0.0000],\n",
      "         [0.0568, 0.9432, 0.0000],\n",
      "         [0.3819, 0.6181, 0.0000]],\n",
      "\n",
      "        [[0.9577, 0.0423, 0.0000],\n",
      "         [0.0300, 0.9700, 0.0000],\n",
      "         [0.3461, 0.6539, 0.0000]],\n",
      "\n",
      "        [[0.8902, 0.1098, 0.0000],\n",
      "         [0.0337, 0.9663, 0.0000],\n",
      "         [0.4624, 0.5376, 0.0000]],\n",
      "\n",
      "        [[0.9358, 0.0642, 0.0000],\n",
      "         [0.0781, 0.9219, 0.0000],\n",
      "         [0.5091, 0.4909, 0.0000]]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([2, 3, 512])\n",
      "sp: torch.Size([2, 3, 512])\n",
      "output----: (tensor([[[ 0.4878,  1.5825, -0.8965,  ..., -1.4015, -0.3449, -1.3370],\n",
      "         [-0.4729, -0.4433,  0.4933,  ..., -0.3028,  0.0753,  0.2583],\n",
      "         [ 1.4655,  2.2277, -1.0216,  ...,  0.0071, -0.4889,  0.4738]],\n",
      "\n",
      "        [[ 0.4878,  1.5825, -0.8965,  ..., -1.4015, -0.3449, -1.3370],\n",
      "         [-0.4729, -0.4433,  0.4933,  ..., -0.3028,  0.0753,  0.2583],\n",
      "         [ 1.4655,  2.2277, -1.0216,  ...,  0.0071, -0.4889,  0.4738]]],\n",
      "       grad_fn=<AddcmulBackward>), [tensor([[[0.9975, 0.0025, 0.0000],\n",
      "         [0.0123, 0.9877, 0.0000],\n",
      "         [0.4650, 0.5350, 0.0000]],\n",
      "\n",
      "        [[0.9912, 0.0088, 0.0000],\n",
      "         [0.0170, 0.9830, 0.0000],\n",
      "         [0.8486, 0.1514, 0.0000]],\n",
      "\n",
      "        [[0.9846, 0.0154, 0.0000],\n",
      "         [0.0232, 0.9768, 0.0000],\n",
      "         [0.7334, 0.2666, 0.0000]],\n",
      "\n",
      "        [[0.9759, 0.0241, 0.0000],\n",
      "         [0.0579, 0.9421, 0.0000],\n",
      "         [0.4594, 0.5406, 0.0000]],\n",
      "\n",
      "        [[0.9726, 0.0274, 0.0000],\n",
      "         [0.0071, 0.9929, 0.0000],\n",
      "         [0.4571, 0.5429, 0.0000]],\n",
      "\n",
      "        [[0.9882, 0.0118, 0.0000],\n",
      "         [0.3546, 0.6454, 0.0000],\n",
      "         [0.6439, 0.3561, 0.0000]],\n",
      "\n",
      "        [[0.7145, 0.2855, 0.0000],\n",
      "         [0.1947, 0.8053, 0.0000],\n",
      "         [0.4024, 0.5976, 0.0000]],\n",
      "\n",
      "        [[0.8223, 0.1777, 0.0000],\n",
      "         [0.2210, 0.7790, 0.0000],\n",
      "         [0.5179, 0.4821, 0.0000]],\n",
      "\n",
      "        [[0.9975, 0.0025, 0.0000],\n",
      "         [0.0123, 0.9877, 0.0000],\n",
      "         [0.4650, 0.5350, 0.0000]],\n",
      "\n",
      "        [[0.9912, 0.0088, 0.0000],\n",
      "         [0.0170, 0.9830, 0.0000],\n",
      "         [0.8486, 0.1514, 0.0000]],\n",
      "\n",
      "        [[0.9846, 0.0154, 0.0000],\n",
      "         [0.0232, 0.9768, 0.0000],\n",
      "         [0.7334, 0.2666, 0.0000]],\n",
      "\n",
      "        [[0.9759, 0.0241, 0.0000],\n",
      "         [0.0579, 0.9421, 0.0000],\n",
      "         [0.4594, 0.5406, 0.0000]],\n",
      "\n",
      "        [[0.9726, 0.0274, 0.0000],\n",
      "         [0.0071, 0.9929, 0.0000],\n",
      "         [0.4571, 0.5429, 0.0000]],\n",
      "\n",
      "        [[0.9882, 0.0118, 0.0000],\n",
      "         [0.3546, 0.6454, 0.0000],\n",
      "         [0.6439, 0.3561, 0.0000]],\n",
      "\n",
      "        [[0.7145, 0.2855, 0.0000],\n",
      "         [0.1947, 0.8053, 0.0000],\n",
      "         [0.4024, 0.5976, 0.0000]],\n",
      "\n",
      "        [[0.8223, 0.1777, 0.0000],\n",
      "         [0.2210, 0.7790, 0.0000],\n",
      "         [0.5179, 0.4821, 0.0000]]], grad_fn=<SoftmaxBackward>), tensor([[[0.9239, 0.0761, 0.0000],\n",
      "         [0.0132, 0.9868, 0.0000],\n",
      "         [0.5177, 0.4823, 0.0000]],\n",
      "\n",
      "        [[0.9695, 0.0305, 0.0000],\n",
      "         [0.1512, 0.8488, 0.0000],\n",
      "         [0.6136, 0.3864, 0.0000]],\n",
      "\n",
      "        [[0.9862, 0.0138, 0.0000],\n",
      "         [0.0221, 0.9779, 0.0000],\n",
      "         [0.5626, 0.4374, 0.0000]],\n",
      "\n",
      "        [[0.9291, 0.0709, 0.0000],\n",
      "         [0.0715, 0.9285, 0.0000],\n",
      "         [0.7564, 0.2436, 0.0000]],\n",
      "\n",
      "        [[0.9360, 0.0640, 0.0000],\n",
      "         [0.1677, 0.8323, 0.0000],\n",
      "         [0.5762, 0.4238, 0.0000]],\n",
      "\n",
      "        [[0.9343, 0.0657, 0.0000],\n",
      "         [0.0505, 0.9495, 0.0000],\n",
      "         [0.4099, 0.5901, 0.0000]],\n",
      "\n",
      "        [[0.9428, 0.0572, 0.0000],\n",
      "         [0.0794, 0.9206, 0.0000],\n",
      "         [0.4730, 0.5270, 0.0000]],\n",
      "\n",
      "        [[0.8791, 0.1209, 0.0000],\n",
      "         [0.0568, 0.9432, 0.0000],\n",
      "         [0.6311, 0.3689, 0.0000]],\n",
      "\n",
      "        [[0.9239, 0.0761, 0.0000],\n",
      "         [0.0132, 0.9868, 0.0000],\n",
      "         [0.5177, 0.4823, 0.0000]],\n",
      "\n",
      "        [[0.9695, 0.0305, 0.0000],\n",
      "         [0.1512, 0.8488, 0.0000],\n",
      "         [0.6136, 0.3864, 0.0000]],\n",
      "\n",
      "        [[0.9862, 0.0138, 0.0000],\n",
      "         [0.0221, 0.9779, 0.0000],\n",
      "         [0.5626, 0.4374, 0.0000]],\n",
      "\n",
      "        [[0.9291, 0.0709, 0.0000],\n",
      "         [0.0715, 0.9285, 0.0000],\n",
      "         [0.7564, 0.2436, 0.0000]],\n",
      "\n",
      "        [[0.9360, 0.0640, 0.0000],\n",
      "         [0.1677, 0.8323, 0.0000],\n",
      "         [0.5762, 0.4238, 0.0000]],\n",
      "\n",
      "        [[0.9343, 0.0657, 0.0000],\n",
      "         [0.0505, 0.9495, 0.0000],\n",
      "         [0.4099, 0.5901, 0.0000]],\n",
      "\n",
      "        [[0.9428, 0.0572, 0.0000],\n",
      "         [0.0794, 0.9206, 0.0000],\n",
      "         [0.4730, 0.5270, 0.0000]],\n",
      "\n",
      "        [[0.8791, 0.1209, 0.0000],\n",
      "         [0.0568, 0.9432, 0.0000],\n",
      "         [0.6311, 0.3689, 0.0000]]], grad_fn=<SoftmaxBackward>), tensor([[[0.9359, 0.0641, 0.0000],\n",
      "         [0.0484, 0.9516, 0.0000],\n",
      "         [0.3526, 0.6474, 0.0000]],\n",
      "\n",
      "        [[0.9564, 0.0436, 0.0000],\n",
      "         [0.0240, 0.9760, 0.0000],\n",
      "         [0.2997, 0.7003, 0.0000]],\n",
      "\n",
      "        [[0.9096, 0.0904, 0.0000],\n",
      "         [0.0946, 0.9054, 0.0000],\n",
      "         [0.4119, 0.5881, 0.0000]],\n",
      "\n",
      "        [[0.8139, 0.1861, 0.0000],\n",
      "         [0.0468, 0.9532, 0.0000],\n",
      "         [0.4880, 0.5120, 0.0000]],\n",
      "\n",
      "        [[0.9132, 0.0868, 0.0000],\n",
      "         [0.0629, 0.9371, 0.0000],\n",
      "         [0.4073, 0.5927, 0.0000]],\n",
      "\n",
      "        [[0.9336, 0.0664, 0.0000],\n",
      "         [0.1190, 0.8810, 0.0000],\n",
      "         [0.6866, 0.3134, 0.0000]],\n",
      "\n",
      "        [[0.8848, 0.1152, 0.0000],\n",
      "         [0.1032, 0.8968, 0.0000],\n",
      "         [0.5508, 0.4492, 0.0000]],\n",
      "\n",
      "        [[0.9280, 0.0720, 0.0000],\n",
      "         [0.0642, 0.9358, 0.0000],\n",
      "         [0.4971, 0.5029, 0.0000]],\n",
      "\n",
      "        [[0.9359, 0.0641, 0.0000],\n",
      "         [0.0484, 0.9516, 0.0000],\n",
      "         [0.3526, 0.6474, 0.0000]],\n",
      "\n",
      "        [[0.9564, 0.0436, 0.0000],\n",
      "         [0.0240, 0.9760, 0.0000],\n",
      "         [0.2997, 0.7003, 0.0000]],\n",
      "\n",
      "        [[0.9096, 0.0904, 0.0000],\n",
      "         [0.0946, 0.9054, 0.0000],\n",
      "         [0.4119, 0.5881, 0.0000]],\n",
      "\n",
      "        [[0.8139, 0.1861, 0.0000],\n",
      "         [0.0468, 0.9532, 0.0000],\n",
      "         [0.4880, 0.5120, 0.0000]],\n",
      "\n",
      "        [[0.9132, 0.0868, 0.0000],\n",
      "         [0.0629, 0.9371, 0.0000],\n",
      "         [0.4073, 0.5927, 0.0000]],\n",
      "\n",
      "        [[0.9336, 0.0664, 0.0000],\n",
      "         [0.1190, 0.8810, 0.0000],\n",
      "         [0.6866, 0.3134, 0.0000]],\n",
      "\n",
      "        [[0.8848, 0.1152, 0.0000],\n",
      "         [0.1032, 0.8968, 0.0000],\n",
      "         [0.5508, 0.4492, 0.0000]],\n",
      "\n",
      "        [[0.9280, 0.0720, 0.0000],\n",
      "         [0.0642, 0.9358, 0.0000],\n",
      "         [0.4971, 0.5029, 0.0000]]], grad_fn=<SoftmaxBackward>), tensor([[[0.9492, 0.0508, 0.0000],\n",
      "         [0.0304, 0.9696, 0.0000],\n",
      "         [0.4278, 0.5722, 0.0000]],\n",
      "\n",
      "        [[0.9731, 0.0269, 0.0000],\n",
      "         [0.0577, 0.9423, 0.0000],\n",
      "         [0.3110, 0.6890, 0.0000]],\n",
      "\n",
      "        [[0.9581, 0.0419, 0.0000],\n",
      "         [0.1001, 0.8999, 0.0000],\n",
      "         [0.3826, 0.6174, 0.0000]],\n",
      "\n",
      "        [[0.9230, 0.0770, 0.0000],\n",
      "         [0.0808, 0.9192, 0.0000],\n",
      "         [0.4630, 0.5370, 0.0000]],\n",
      "\n",
      "        [[0.9580, 0.0420, 0.0000],\n",
      "         [0.0332, 0.9668, 0.0000],\n",
      "         [0.5391, 0.4609, 0.0000]],\n",
      "\n",
      "        [[0.9388, 0.0612, 0.0000],\n",
      "         [0.1157, 0.8843, 0.0000],\n",
      "         [0.3848, 0.6152, 0.0000]],\n",
      "\n",
      "        [[0.8818, 0.1182, 0.0000],\n",
      "         [0.1371, 0.8629, 0.0000],\n",
      "         [0.4630, 0.5370, 0.0000]],\n",
      "\n",
      "        [[0.9620, 0.0380, 0.0000],\n",
      "         [0.0248, 0.9752, 0.0000],\n",
      "         [0.3115, 0.6885, 0.0000]],\n",
      "\n",
      "        [[0.9492, 0.0508, 0.0000],\n",
      "         [0.0304, 0.9696, 0.0000],\n",
      "         [0.4278, 0.5722, 0.0000]],\n",
      "\n",
      "        [[0.9731, 0.0269, 0.0000],\n",
      "         [0.0577, 0.9423, 0.0000],\n",
      "         [0.3110, 0.6890, 0.0000]],\n",
      "\n",
      "        [[0.9581, 0.0419, 0.0000],\n",
      "         [0.1001, 0.8999, 0.0000],\n",
      "         [0.3826, 0.6174, 0.0000]],\n",
      "\n",
      "        [[0.9230, 0.0770, 0.0000],\n",
      "         [0.0808, 0.9192, 0.0000],\n",
      "         [0.4630, 0.5370, 0.0000]],\n",
      "\n",
      "        [[0.9580, 0.0420, 0.0000],\n",
      "         [0.0332, 0.9668, 0.0000],\n",
      "         [0.5391, 0.4609, 0.0000]],\n",
      "\n",
      "        [[0.9388, 0.0612, 0.0000],\n",
      "         [0.1157, 0.8843, 0.0000],\n",
      "         [0.3848, 0.6152, 0.0000]],\n",
      "\n",
      "        [[0.8818, 0.1182, 0.0000],\n",
      "         [0.1371, 0.8629, 0.0000],\n",
      "         [0.4630, 0.5370, 0.0000]],\n",
      "\n",
      "        [[0.9620, 0.0380, 0.0000],\n",
      "         [0.0248, 0.9752, 0.0000],\n",
      "         [0.3115, 0.6885, 0.0000]]], grad_fn=<SoftmaxBackward>), tensor([[[0.9515, 0.0485, 0.0000],\n",
      "         [0.0558, 0.9442, 0.0000],\n",
      "         [0.4682, 0.5318, 0.0000]],\n",
      "\n",
      "        [[0.9840, 0.0160, 0.0000],\n",
      "         [0.0412, 0.9588, 0.0000],\n",
      "         [0.6297, 0.3703, 0.0000]],\n",
      "\n",
      "        [[0.9512, 0.0488, 0.0000],\n",
      "         [0.0753, 0.9247, 0.0000],\n",
      "         [0.5067, 0.4933, 0.0000]],\n",
      "\n",
      "        [[0.9429, 0.0571, 0.0000],\n",
      "         [0.0305, 0.9695, 0.0000],\n",
      "         [0.5267, 0.4733, 0.0000]],\n",
      "\n",
      "        [[0.8916, 0.1084, 0.0000],\n",
      "         [0.1315, 0.8685, 0.0000],\n",
      "         [0.4825, 0.5175, 0.0000]],\n",
      "\n",
      "        [[0.9379, 0.0621, 0.0000],\n",
      "         [0.0409, 0.9591, 0.0000],\n",
      "         [0.3608, 0.6392, 0.0000]],\n",
      "\n",
      "        [[0.9642, 0.0358, 0.0000],\n",
      "         [0.0133, 0.9867, 0.0000],\n",
      "         [0.5999, 0.4001, 0.0000]],\n",
      "\n",
      "        [[0.9095, 0.0905, 0.0000],\n",
      "         [0.0551, 0.9449, 0.0000],\n",
      "         [0.4523, 0.5477, 0.0000]],\n",
      "\n",
      "        [[0.9515, 0.0485, 0.0000],\n",
      "         [0.0558, 0.9442, 0.0000],\n",
      "         [0.4682, 0.5318, 0.0000]],\n",
      "\n",
      "        [[0.9840, 0.0160, 0.0000],\n",
      "         [0.0412, 0.9588, 0.0000],\n",
      "         [0.6297, 0.3703, 0.0000]],\n",
      "\n",
      "        [[0.9512, 0.0488, 0.0000],\n",
      "         [0.0753, 0.9247, 0.0000],\n",
      "         [0.5067, 0.4933, 0.0000]],\n",
      "\n",
      "        [[0.9429, 0.0571, 0.0000],\n",
      "         [0.0305, 0.9695, 0.0000],\n",
      "         [0.5267, 0.4733, 0.0000]],\n",
      "\n",
      "        [[0.8916, 0.1084, 0.0000],\n",
      "         [0.1315, 0.8685, 0.0000],\n",
      "         [0.4825, 0.5175, 0.0000]],\n",
      "\n",
      "        [[0.9379, 0.0621, 0.0000],\n",
      "         [0.0409, 0.9591, 0.0000],\n",
      "         [0.3608, 0.6392, 0.0000]],\n",
      "\n",
      "        [[0.9642, 0.0358, 0.0000],\n",
      "         [0.0133, 0.9867, 0.0000],\n",
      "         [0.5999, 0.4001, 0.0000]],\n",
      "\n",
      "        [[0.9095, 0.0905, 0.0000],\n",
      "         [0.0551, 0.9449, 0.0000],\n",
      "         [0.4523, 0.5477, 0.0000]]], grad_fn=<SoftmaxBackward>), tensor([[[0.8944, 0.1056, 0.0000],\n",
      "         [0.0595, 0.9405, 0.0000],\n",
      "         [0.6063, 0.3937, 0.0000]],\n",
      "\n",
      "        [[0.9716, 0.0284, 0.0000],\n",
      "         [0.0721, 0.9279, 0.0000],\n",
      "         [0.4712, 0.5288, 0.0000]],\n",
      "\n",
      "        [[0.9221, 0.0779, 0.0000],\n",
      "         [0.0832, 0.9168, 0.0000],\n",
      "         [0.4269, 0.5731, 0.0000]],\n",
      "\n",
      "        [[0.9640, 0.0360, 0.0000],\n",
      "         [0.0920, 0.9080, 0.0000],\n",
      "         [0.4228, 0.5772, 0.0000]],\n",
      "\n",
      "        [[0.8727, 0.1273, 0.0000],\n",
      "         [0.0568, 0.9432, 0.0000],\n",
      "         [0.3819, 0.6181, 0.0000]],\n",
      "\n",
      "        [[0.9577, 0.0423, 0.0000],\n",
      "         [0.0300, 0.9700, 0.0000],\n",
      "         [0.3461, 0.6539, 0.0000]],\n",
      "\n",
      "        [[0.8902, 0.1098, 0.0000],\n",
      "         [0.0337, 0.9663, 0.0000],\n",
      "         [0.4624, 0.5376, 0.0000]],\n",
      "\n",
      "        [[0.9358, 0.0642, 0.0000],\n",
      "         [0.0781, 0.9219, 0.0000],\n",
      "         [0.5091, 0.4909, 0.0000]],\n",
      "\n",
      "        [[0.8944, 0.1056, 0.0000],\n",
      "         [0.0595, 0.9405, 0.0000],\n",
      "         [0.6063, 0.3937, 0.0000]],\n",
      "\n",
      "        [[0.9716, 0.0284, 0.0000],\n",
      "         [0.0721, 0.9279, 0.0000],\n",
      "         [0.4712, 0.5288, 0.0000]],\n",
      "\n",
      "        [[0.9221, 0.0779, 0.0000],\n",
      "         [0.0832, 0.9168, 0.0000],\n",
      "         [0.4269, 0.5731, 0.0000]],\n",
      "\n",
      "        [[0.9640, 0.0360, 0.0000],\n",
      "         [0.0920, 0.9080, 0.0000],\n",
      "         [0.4228, 0.5772, 0.0000]],\n",
      "\n",
      "        [[0.8727, 0.1273, 0.0000],\n",
      "         [0.0568, 0.9432, 0.0000],\n",
      "         [0.3819, 0.6181, 0.0000]],\n",
      "\n",
      "        [[0.9577, 0.0423, 0.0000],\n",
      "         [0.0300, 0.9700, 0.0000],\n",
      "         [0.3461, 0.6539, 0.0000]],\n",
      "\n",
      "        [[0.8902, 0.1098, 0.0000],\n",
      "         [0.0337, 0.9663, 0.0000],\n",
      "         [0.4624, 0.5376, 0.0000]],\n",
      "\n",
      "        [[0.9358, 0.0642, 0.0000],\n",
      "         [0.0781, 0.9219, 0.0000],\n",
      "         [0.5091, 0.4909, 0.0000]]], grad_fn=<SoftmaxBackward>)])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(5,6,512)\n",
    "vocab = {\"some\": 1, \"words\": 2, \"pad\":0}\n",
    "word_index = torch.Tensor([[vocab[w] for w in [\"some\", \"words\", \"pad\"]]for _ in range(2)])\n",
    "input = word_index.to(torch.int64)# word2index = [X[w] for w in [\"some\", \"words\"]]\n",
    "\n",
    "# X = torch.tensor(X).to(torch.float64)\n",
    "encoder = Encoder()\n",
    "# print(\"X:\", X)\n",
    "print(\"input:\", input.shape)\n",
    "inputs_len = torch.tensor([[len(seq)] for seq in input])\n",
    "print(\"inputs_len:\", inputs_len.shape)\n",
    "output = encoder(input, inputs_len) #实例调用encoder()(...)\n",
    "print(\"output----:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "7b5e6192-8bd7-4d46-b840-1a20548498c5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
